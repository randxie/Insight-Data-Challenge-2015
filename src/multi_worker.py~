__author__ = 'yxie'
from feature_manager import feature_manager, find_median
import cPickle as pickle
import os, math, bisect, logging, time
from common_var import output_dir, feature2_txt_name, feature1_txt_name, storage_dir_txt, storage_dir_p
from common_var import head_file, tail_file, f_para, num_of_worker, num_of_split_file
from multiprocessing.dummy import Pool as ThreadPool
from support_fun import get_file_line

class multi_worker():
    # initialization
    def __init__(self,s_para):
        self.main_f_manager=feature_manager(s_para)
        self.last_pos=self.main_f_manager.get_last_pos()
        self.total_storage_name=s_para['storage_filename']
        self.par_f_manager={}
        self.total_word_storage=self.main_f_manager.word_storage
        self.total_median_arr=self.main_f_manager.median_arr
        self.total_indiv_word_arr=self.main_f_manager.indiv_word_arr
        self.total_unsorted_indiv_word_arr=self.main_f_manager.unsorted_indiv_word_arr

    # split txt into different files
    def multi_split_txt(self,filename,split_num_line):
        self.add_logging('splitting the txt into small pieces')
        s1,s2 = os.path.splitext(filename)
        os.system('split --lines=%d --numeric-suffixes --suffix-length=%d %s %s' %(split_num_line,self.suffix_length,filename,s1))

    # calculate feature using multiple workers
    def multi_calculate_feature(self,f_para):
        self.add_logging('Run things in parallel')
        start_time = time.time()
        in_file=f_para['in_file']
        num_lines=get_file_line(in_file)
        # cut file in to head(analyzed before) and tail(haven't been analyzed)
        if not self.check_head_tail():
            os.system('head -n %d %s > %s' %(self.last_pos,in_file,head_file))
            os.system('tail -n %d %s > %s'%(num_lines-self.last_pos,in_file,tail_file))
        else:
            self.add_logging('head.txt and tail.txt exists, finish finished un-done job firstly')
        # un-analyzed line numbers
        tail_num_lines=get_file_line(tail_file)
        split_num_line=math.ceil(tail_num_lines*1.0/num_of_split_file)
        if split_num_line:
            self.suffix_length=len(str(int(tail_num_lines*1.0/split_num_line)))
            if not self.check_split():
                self.add_logging('tail.txt already splitted, finish un-done job firstly')
                self.multi_split_txt(tail_file,split_num_line)
            self.get_split_file_num()
            self.process_data_par()
            self.concatenate_results()
        elapsed_time = time.time() - start_time
        self.add_logging('Total feature calculation takes time: %s second' %str(elapsed_time))
        self.save_feature()

    # clean temporary files
    def clean_tmp_file(self):
        os.system('rm %s'%(os.path.join(os.getcwd(),storage_dir_txt,'*')))
        os.system('rm %s'%(os.path.join(os.getcwd(),storage_dir_p,'*')))

    # calculate features in parallel
    def process_data_par(self):
        logging.info ('start to calculate feature in parallel')
        s_para={}
        for i in range(0,self.num_of_file):
            tmpstr=(str(i).zfill(self.suffix_length))
            storage_file_p='data_storage_%s.p' %tmpstr
            s_para['storage_filename']=os.path.join(storage_dir_p,storage_file_p)
            s_para['worker_no']='worker %d' %i
            self.par_f_manager[tmpstr]=feature_manager(s_para)
        # process data in parallel
        p = ThreadPool(num_of_worker)
        l=range(0,self.num_of_file)
        result=p.map(self.let_worker_run,l)
        p.close()
        p.join()

    def check_head_tail(self):
        for name in os.listdir(os.path.join(os.getcwd(),storage_dir_txt)):
            if 'head.txt' in name:
                return True
        return False

    def check_split(self):
        for name in os.listdir(os.path.join(os.getcwd(),storage_dir_p)):
            if 'data_storage' in name:
                return True
        return False

    # count number of txt file (other than head.txt and tail.txt)
    def get_split_file_num(self):
        tmp_num=0
        for name in os.listdir(os.path.join(os.getcwd(),storage_dir_txt)):
            if ('tail' in name) and name!='tail.txt':
                tmp_num=tmp_num+1
        self.num_of_file=tmp_num

    # the ith worker will be assigned the ith txt
    def let_worker_run(self,i):
        f_para_tmp={}
        f_para_tmp['line_to_print']=f_para['line_to_print']
        f_para_tmp['line_to_save']=5*f_para['line_to_print']    #save intermediate calculation result every XXXX lines
        tmpstr=str(i).zfill(self.suffix_length)
        in_file_tmp='tail%s' %tmpstr
        in_filename=os.path.join(os.getcwd(),storage_dir_txt,in_file_tmp)
        f_para_tmp['in_file']=in_filename
        self.par_f_manager[tmpstr].calculate_feature(f_para_tmp)

    # concatenate all the results
    def concatenate_results(self):
        for i in range(0,self.num_of_file):
            tmpstr=(str(i).zfill(self.suffix_length))
            self.concatenate_word_dict(self.par_f_manager[tmpstr].word_storage)
            self.concatenate_median(self.par_f_manager[tmpstr].unsorted_indiv_word_arr)
            self.last_pos=self.last_pos+self.par_f_manager[tmpstr].get_last_pos()

    # concatenate calculated word dictionary one by one
    def concatenate_word_dict(self,new_dict):
        for key,value in new_dict.items():
            if key in self.total_word_storage:
                self.total_word_storage[key]=self.total_word_storage[key]+value
            else:
                self.total_word_storage[key]=value

    # concatenate calculated median vectors one by one
    def concatenate_median(self,new_unsorted):
        self.total_unsorted_indiv_word_arr.extend(new_unsorted)
        for i in range(len(new_unsorted)):
            bisect.insort(self.total_indiv_word_arr,new_unsorted[i])
            self.total_median_arr.append(find_median(self.total_indiv_word_arr))

    # save temporary variable for feature calculation
    def save_feature(self):
        tmpdict={'word_storage':self.total_word_storage,'indiv_word_arr':self.total_indiv_word_arr,\
                'median_arr':self.total_median_arr,'last_word_pos':self.last_pos,\
                'unsorted_indiv_word_arr':self.unsorted_indiv_word_arr}
        pickle.dump(tmpdict,open(os.path.join(os.getcwd(),self.total_storage_name),'wb'))

    def add_logging(self,str):
        logging.info('multi-worker: '+str)
